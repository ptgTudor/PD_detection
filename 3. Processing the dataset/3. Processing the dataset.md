# 3. Processing the dataset

Splitting the files was done randomly through the splitfolders library, taking into account that if speakers submitted multiple recordings for an experiment, all their files must be in the same dataset, whether it be training, validation or testing. The datasets were created from the directories resulted from the splitting process.

![image](https://github.com/user-attachments/assets/208c5bec-0882-4c48-a2ee-8883e06129e1)\
*Creation of the datasets*

The batch size is a hyperparameter that quantifies the number of files taken from the training set in one iteration of the training process, whose value is 32. The seed parameter shuffles the data in a certain way, while the output_sequence_length parameter indicates the length of the audio sequences, padding files shorter than that to the chosen length. Here, after converting the files to 16 kHz, having an output sequence length of 16000 means 1 second will be taken from the files, since this is the shortest duration of files present in the dataset.

Above, two waveforms from the “pataka” experiment can be seen, one in its entire length and the other one cut at 1 second. The waveforms below are from the same experiment, with one waveform cut at 1 second and the other being the augmented version of it. Its amplitude values were clipped at ±1 to avoid distortion and maintain a standard range.

As it was mentioned earlier in the paper, a lot of the information pertinent to the diagnosis is contained in frequency changes. Thus, it was deemed appropriate to convert the waveforms into spectrograms and have these fed as the dataset into the model. Below the functions to obtain and plot a spectrogram are highlighted, with some examples of spectrograms obtained from the previous waveforms:
