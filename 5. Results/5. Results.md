# Results

For the DDK experiments, the dataset contains 100 samples, one from each participant, so the chosen training-validation-testing splits were 60-20-20 and 80-10-10. Each training file was augmented 10 times, meaning 600 or 800 training files. For this amount, working with a batch size of 32 was deemed appropriate.

Three learning rate variations were tested: a constant learning rate of 0.001, a learning rate that decreases every 20 epochs (out of a total of 100) tenfold and an exponentially decaying learning rate that decreases tenfold at an adjustable number of epochs.

A number of 5 tests were ran for each combination of a training-validation-testing split and learning rate method for which the accuracy, precision, recall and F1 score values were recorded, then an average was computed for every type of test.

The complete results are listed in the appendix, but here are some observations:
• The learning rate scheduler and exponential decay methods either provided similar results to those of a constant learning rate set at 0.001, or even worse in some cases.
•For the “pataka”, “petaka” and “pakata” tests, a training-validation-testing split of 80-10-10 provided better accuracies than 60-20-20, even though penalties for wrong predictions tend to be higher when the number of testing samples is small. This is possibly due to the variety in syllables and the need for more training samples.
•For the “pa-pa-pa”, “ta-ta-ta” and “ka-ka-ka” tests, a training-validation-testing split of 60-20-20 provided better accuracies than 80-10-10, possibly due to the fact that only one syllable is being uttered in this case so the model has enough training data even at 60%.
•Precision scores were generally high, which means that the model does not tend to classify healthy controls as PD patients. Recall scores were generally lower, which indicates that issues arise when identifying true positives in the model, meaning that more traits specific to PD patients might need to be captured.
•The highest accuracy was obtained for the “ka-ka-ka” test, 76% with constant learning rate set at 0.001 and 60-20-20 splits, as well as with a learning rate scheduler and 80-10-10 splits.
•The lowest accuracy was recorded for the “ta-ta-ta” test, 49% with a learning rate scheduler and 60-20-20 splits. This test, along with the “pa-pa-pa” one, provided the worst accuracies in general.
•The “pataka”, “petaka”, “pakata” and “ka-ka-ka” tests provided the best accuracies.

For the vowel experiments, the dataset contains 300 samples, three from each participant, so the chosen training-validation-testing split was 80-10-10, since the number of files for testing is now large enough to get a clearer view of the model. Each training file was augmented 10 times, meaning 2400 training files. For this amount, working with a batch size of 64 was deemed appropriate.

The learning rate methods were dropped, as they did not warrant better results than simply using a constant learning rate. A number of 3 tests were ran for each vowel, with a learning rate of 0.001 over 100 epochs. The accuracy, precision, recall and F1 score values were recorded, then an average was computed for every vowel.

The complete results are listed in the appendix, but here are some observations:
• The “A” vowel provided the best results by a large margin, with 69% accuracy, 74% precision and 58% recall, meaning that the model succeeded in diagnosing a considerable number of patients with the disease, but also misdiagnosed a few healthy controls.
• The “E”, “I” and “O” vowels had accuracies slightly higher than 50%, with precision scores above 50% but not above 80%, meaning that false positives were prevalent.
• The “U” vowel had an accuracy of 48%, which is the lowest result recorded here, indicating that it is not suited for this analysis.

Throughout tests recorded on the web application, ten people without any symptoms associated with the disease were tested. For the DDK experiments, the “ka-ka-ka”, “pataka” and “petaka” tests had 100% accuracy, while the “pa-pa-pa” test had 90% accuracy and the “ta-ta-ta” test had 70% accuracy. The worst accuracy was returned by the “pakata” test, with 40%. This indicates some variability around the results given by testing on the database, although this is to be expected when exposing the model to unseen data.

The vowel experiments also had results quite different to the simulations. While the “A” vowel test had an accuracy of 70%, almost identical to the one in the simulations, and the “U” vowel test recorded the lowest results here as well, at 40%, the “E” and “O” tests had accuracies of 90% and 100% respectively. The “I” test was close to the simulation values at around 60%.

The number of positive diagnoses was between 0 and 4, with one person recording no positive diagnosis and 2 people recording 4 positive diagnoses. One positive diagnosis was assigned to one person and the rest had either 2 or 3 positive samples, with most of these coming from the vowel tests, which indicates the need to further improve the algorithm behind them.