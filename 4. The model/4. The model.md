# 4. The model

The model is based on a kind of deep neural network, namely a convolutional neural network because it employs convolutional layers, making it more suitable for image analysis due to its ability to learn patterns. It is highlighted in the image below.

It is a sequential model, allowing the stacking of layers in a sequential manner. Firstly, an input layer is applied, which tells the model what kind of shapes to expect from its input data, represented by the spectrograms. Afterwards, a normalization layer is applied, which standardizes the input data by computing the mean and variance across the features of the input and subtracting the mean from the features of the input and dividing by the standard deviation, creating a standard distribution, reducing the possible high impact of certain features and leading to better generalization of the data.

Afterwards, three sequences consisting of a 2D convolutional layer, a 2D MaxPooling layer and a dropout layer are applied. The role of convolutional layers is to extract features from the input data, doing so through its parameters: the number of filters, which detect different features of the input, the kernel size, which is the size of the filter that slides over the input image and applies the convolution operation, the activation function (“ReLU” in this case, standing for Rectified Linear Unit), which is applied to the output of the convolution operation by replacing all negative pixel values with zero as a countermeasure to the vanishing gradient problem and a kernel regularizer, which prevents overfitting by introducing a penalty to the loss function to tackle weight propagation throughout the layers, ensuring the model’s convergence. The L2 regularization chosen here considers the squares of the weights of the layers, keeping related features, as opposed to L1 regularization which is preferred in feature selection. The number of filters for each of these layers was increased progressively to start from capturing low level features in the spectrograms and then at the end capture the high level ones.

The MaxPooling layers downsample the input data, gradually reducing its dimensions. The process behind them is based on moving a 2x2 window (which is the default value) over the data and selecting the maximum value in that window, helping the model to generalize better and to focus on the most important features.

The Dropout layers randomly set a percentage of the input pixels to zero (in this case 50%), being another measure against overfitting and ensuring that the model does not focus too much on individual features.

The Flatten layer converts a 2D array map into a 1D array, with the aim of feeding it to a dense layer, transitioning from convolutional feature extraction to decision making. All the neurons in dense layers are connected to all neurons in the previous layer, in this case the flattening one, enabling the observation of more complex features. The last layer is also a dense layer, which provides the final output, taking into account all of the features captured up until that point.

Afterwards, the variation of the learning rate was explored. By definition, it is a hyperparameter that controls the adjustment rate of the model according to the losses. This variation was explored to avoid stagnation in case there would be more features to learn that would remain unknown with a constant value through two methods: scheduling learning rates for a chosen number of epochs and exponentially decaying learning rates after a number of steps, computed from the total number of samples, the batch size and the number of epochs.

The Adam (Adaptive Moment Estimation) algorithm was chosen because it allows for adaptive learning rates, it keeps track of gradient descents and corrects biases that may appear at the start of the training process.

The Sparse Categorical Cross Entropy losses were chosen because of a better fit with mutually exclusive classes.
